# union_chatbot_app.py
# -*- coding: utf-8 -*-

import os
import sys
import re
import hashlib
from pathlib import Path
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.schema import Document, HumanMessage
from langchain.chains import RetrievalQA

# ğŸ¯ 1. Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# âœ… 2. ì•ˆì „í•œ ìœ ë‹ˆì½”ë“œ ì •ë¦¬ í•¨ìˆ˜
def safe_unicode(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")

# âœ… 3. PDF ì „ì²˜ë¦¬ìš© ë¬¸ìì—´ í´ë¦¬ë„ˆ
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.replace("\x00", "")
    text = re.sub(r"[\u0000-\u001F\u007F-\u009F]", "", text)
    text = re.sub(r"[\ud800-\udfff]", "", text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore").strip()

# âœ… 4. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(path: Path) -> str:
    try:
        reader = PdfReader(str(path))
        pages = []
        for page in reader.pages:
            raw = page.extract_text() or ""
            pages.append(clean_text(raw))
        return "\n".join(pages)
    except Exception as e:
        st.warning(f"[PDF ì¶”ì¶œ ì‹¤íŒ¨] {path.name}: {e}")
        return ""

# âœ… 5. íŒŒì¼ í•´ì‹œ
def compute_file_hash(file_paths):
    hash_md5 = hashlib.md5()
    for path in sorted(file_paths):
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
    return hash_md5.hexdigest()

# âœ… 6. ë¬¸ì„œ ë¡œë”©
@st.cache_resource
def load_all_documents_with_hash(pdf_paths, file_hash):
    documents = []
    for path in pdf_paths:
        text = extract_text_from_pdf(path)
        if text.strip():
            doc = Document(page_content=text, metadata={"source": str(path.name)})
            documents.append(doc)
        else:
            st.warning(f"[ìƒëµ] {path.name} ì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return documents

# âœ… 7. chunk ë¶„ë¦¬
@st.cache_resource
def split_documents_into_chunks(documents):
    total_length = sum(len(doc.page_content) for doc in documents)
    avg_length = total_length // len(documents) if documents else 0

    if avg_length > 6000:
        chunk_size, overlap = 1500, 300
    elif avg_length > 3000:
        chunk_size, overlap = 1000, 200
    else:
        chunk_size, overlap = 700, 100

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    return splitter.split_documents(documents)

# âœ… 8. FAISS ë²¡í„° DB
@st.cache_resource
def create_vector_store(chunks, embedding_model):
    try:
        for doc in chunks:
            doc.page_content = safe_unicode(doc.page_content)
        return FAISS.from_documents(chunks, embedding_model)
    except Exception as e:
        st.error(f"âŒ FAISS ë²¡í„° DB ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

# âœ… 9. QA ì²´ì¸
@st.cache_resource
def initialize_qa_chain(pdf_paths):
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    file_hash = compute_file_hash(pdf_paths)
    docs = load_all_documents_with_hash(pdf_paths, file_hash)
    if not docs:
        st.error("PDF ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    chunks = split_documents_into_chunks(docs)
    db = create_vector_store(chunks, embeddings)
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 6})
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4o", temperature=0)
    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)

# âœ… 10. ì§ˆë¬¸ í™•ì¥
@st.cache_resource
def get_query_expander():
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4o", temperature=0)
    def expand(query: str) -> str:
        try:
            prompt = HumanMessage(content=safe_unicode(
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ PDF ë‚´ìš©ê³¼ ì˜ ë§¤ì¹­ë˜ë„ë¡ êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ë°”ê¿”ì¤˜. "
                "PDF ìš©ì–´ì™€ í‘œí˜„ì„ ë°˜ì˜í•˜ê³ , í•„ìš”í•œ ë¶€ì—°ë„ ì¶”ê°€í•´ë„ ì¢‹ì•„. "
                f"ì§ˆë¬¸: {query}"
            ))
            response = llm.invoke([prompt])
            return safe_unicode(response.content.strip())
        except Exception as e:
            st.warning(f"â• ì§ˆë¬¸ í™•ì¥ ì‹¤íŒ¨: {e!r}")
            return query
    return expand
