import os
import streamlit as st
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# ğŸ”‘ OpenAI API í‚¤ ì„¤ì •
openai_api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. secrets.toml ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì— í‚¤ë¥¼ ë“±ë¡í•˜ì„¸ìš”.")
    st.stop()

# âœ… í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ (ì„ë² ë”© ì˜¤ë¥˜ ë°©ì§€)
def sanitize(text):
    return text.encode("utf-8", "ignore").decode("utf-8", "ignore")

# ğŸ“‚ PDF ë¬¸ì„œ ë¡œë”©
@st.cache_resource
def load_documents():
    loaders = [
        PyPDFLoader("./data/labor_policy_agenda_250627.pdf"),
        PyPDFLoader("./data/samsung_me_union_meeting_250704.pdf"),
        PyPDFLoader("./data/external_audit_250204.pdf")
    ]
    documents = []
    for loader in loaders:
        documents.extend(loader.load())
    return documents

# ğŸ§  ë²¡í„° DB ìƒì„±
@st.cache_resource(show_spinner="ğŸ“š ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
def create_vector_db():
    documents = load_documents()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = splitter.split_documents(documents)
    cleaned_texts = [sanitize(t.page_content) for t in texts if t.page_content.strip()]
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    return FAISS.from_texts(cleaned_texts, embeddings)

# ğŸ¤– QA ì²´ì¸ êµ¬ì„±
@st.cache_resource
def get_qa_chain():
    vectorstore = create_vector_db()
    retriever = vectorstore.as_retriever()
    return RetrievalQA.from_chain_type(
        llm=ChatOpenAI(openai_api_key=openai_api_key, temperature=0),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )

# ğŸ–¼ï¸ ìƒë‹¨ UI
st.image("1.png", width=110)
st.markdown("<h1 style='display:inline-block; margin-left:10px;'>ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ë™ì¡°í•© ìƒë‹´ì‚¬</h1>", unsafe_allow_html=True)
st.write("ë…¸ì¡° ê´€ë ¨ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.")

# ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?", placeholder="ì˜ˆ: ì™¸ë¶€ ê°ì‚¬ ì¤€ë¹„ í•­ëª©ì€?")

if query:
    with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
        qa = get_qa_chain()
        result = qa(query)
        st.success(result["result"])

        with st.expander("ğŸ“ ê´€ë ¨ ë¬¸ì„œ ë³´ê¸°"):
            for i, doc in enumerate(result["source_documents"]):
                st.markdown(f"**ë¬¸ì„œ {i+1}:** {doc.metadata.get('source', '')}")
                st.write(doc.page_content[:1000])
