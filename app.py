# app.py - ì „ì²´ ë¦¬íŒ©í„°ë§ í†µí•© ë²„ì „

import os
import sys
import re
import hashlib
from pathlib import Path
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.schema import Document, HumanMessage
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# âœ… ìœ ë‹ˆì½”ë“œ ë° ë¬¸ìì—´ ì²˜ë¦¬

def safe_unicode(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.replace("\x00", "")
    text = re.sub(r"[\u0000-\u001F\u007F-\u009F]", "", text)
    text = re.sub(r"[\ud800-\udfff]", "", text)
    return safe_unicode(text.strip())

# âœ… PDF í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

def extract_text_from_pdf(path: Path) -> list:
    try:
        reader = PdfReader(str(path))
        pages = []
        for i, page in enumerate(reader.pages):
            raw = page.extract_text() or ""
            pages.append(Document(
                page_content=clean_text(raw),
                metadata={"source": path.name, "page_number": i + 1}
            ))
        return pages
    except Exception as e:
        st.warning(f"[PDF ì¶”ì¶œ ì‹¤íŒ¨] {path.name}: {e}")
        return []

# âœ… í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€ ìºì‹±

def compute_file_hash(file_paths):
    hash_md5 = hashlib.md5()
    for path in sorted(file_paths):
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
    return hash_md5.hexdigest()

@st.cache_resource
def load_documents_with_metadata(pdf_paths):
    documents = []
    for path in pdf_paths:
        pages = extract_text_from_pdf(path)
        documents.extend(pages)
    return documents

# âœ… chunk í¬ê¸° ì¡°ì • (ìë™ + ìˆ˜ë™)

def get_chunk_config(documents):
    total_length = sum(len(doc.page_content) for doc in documents)
    avg_length = total_length // len(documents) if documents else 0
    if avg_length > 6000:
        return 1500, 300
    elif avg_length > 3000:
        return 1000, 200
    else:
        return 700, 200

@st.cache_resource
def split_documents_into_chunks(documents, chunk_size, overlap):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    return splitter.split_documents(documents)

# âœ… ë²¡í„° DB ìƒì„±

def initialize_vector_store(chunks, api_key):
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    for doc in chunks:
        doc.page_content = safe_unicode(doc.page_content)
    return FAISS.from_documents(chunks, embeddings)

# âœ… ì§ˆë¬¸ í™•ì¥

def get_query_expander(api_key):
    llm = ChatOpenAI(openai_api_key=api_key, model_name="gpt-4o", temperature=0)
    def expand(query: str) -> str:
        try:
            prompt = HumanMessage(content=safe_unicode(
                "ë‹¤ìŒ ë¬¸ì¥ì„ PDF ê²€ìƒ‰ì— ìµœì í™”ë˜ë„ë¡ ë°”ê¿”ì¤˜. PDF ë‚´ ìì£¼ ë‚˜ì˜¤ëŠ” í‘œí˜„ì´ë‚˜ ë¬¸ì¥ í˜•íƒœë¡œ ì¬ì‘ì„±í•˜ê³ , ë¬¸ë§¥ì´ ì• ë§¤í•˜ë©´ ë¶€ì—° ì„¤ëª…ë„ ë„£ì–´ì¤˜.\n"
                f"ì§ˆë¬¸: {query}"
            ))
            response = llm.invoke([prompt])
            return safe_unicode(response.content.strip())
        except Exception as e:
            st.warning(f"â• ì§ˆë¬¸ í™•ì¥ ì‹¤íŒ¨: {safe_unicode(str(e))}")
            return query
    return expand

# âœ… QA ì²´ì¸ ìƒì„±

def initialize_qa_chain(vector_store, api_key):
    system_prompt = """
ë„ˆëŠ” ë°˜ë“œì‹œ PDF ë¬¸ì„œì— í¬í•¨ëœ ë‚´ìš©ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µí•´ì•¼ í•´.
ë¬¸ì„œì— ëª…ì‹œì  ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ì• ë§¤í•œ ê²½ìš° 'ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•´.
"""
    qa_prompt = PromptTemplate(
        template=system_prompt + "\n\n{context}\n\nì§ˆë¬¸: {question}\në‹µë³€:",
        input_variables=["context", "question"]
    )
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 15})
    llm = ChatOpenAI(openai_api_key=api_key, model_name="gpt-4o", temperature=0)
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": qa_prompt},
        return_source_documents=True
    )

# âœ… Streamlit UI êµ¬ì„±
st.set_page_config(page_title="ì‚¼ì„±ì „ê¸° ì¢…ì¤‘ë…¸ì¡° ìƒë‹´ì‚¬", layout="centered", page_icon="logo_union_hands.png")
st.markdown("""
<div style='display: flex; align-items: center; justify-content: center; gap: 10px; margin-bottom: 10px;'>
    <img src="https://raw.githubusercontent.com/SHIN-HOON-SIK/semunion-chatbot/main/logo_union_hands.png" width="50"/>
    <h1 style='color: #0d1a44; margin: 0;'>ì‚¼ì„±ì „ê¸° ì¢…ì¤‘ë…¸ì¡° ìƒë‹´ì‚¬</h1>
</div>
""", unsafe_allow_html=True)

st.write("PDF ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì— ëŒ€í•´ GPTê°€ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

openai_api_key = os.getenv("OPENAI_API_KEY", "").strip()
if not openai_api_key:
    try:
        openai_api_key = st.secrets["OPENAI_API_KEY"]
    except Exception:
        st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

base_dir = Path(__file__).parent
pdf_dir = base_dir / "data"
pdf_files = ["policy_agenda_250627.pdf", "union_meeting_250704.pdf", "SEMUNION_DATA_BASE.pdf"]
pdf_paths = [pdf_dir / name for name in pdf_files]

try:
    documents = load_documents_with_metadata(pdf_paths)
    chunk_size, overlap = get_chunk_config(documents)
    chunk_size = st.sidebar.slider("Chunk í¬ê¸°", 300, 2000, chunk_size, step=100)
    overlap = st.sidebar.slider("Overlap", 0, 500, overlap, step=50)
    chunks = split_documents_into_chunks(documents, chunk_size, overlap)
    vector_store = initialize_vector_store(chunks, openai_api_key)
    query_expander = get_query_expander(openai_api_key)
    qa_chain = initialize_qa_chain(vector_store, openai_api_key)
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {safe_unicode(str(e))}")
    st.stop()

user_query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹œë‚˜ìš”?", placeholder="ì˜ˆ: ì§‘í–‰ë¶€ êµ¬ì„±ì€?")
if user_query.strip():
    query = query_expander(user_query)
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            result = qa_chain.invoke({"query": query})
            answer = safe_unicode(result["result"])
            if not answer or "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in answer:
                st.info("ë¬¸ì„œ ë‚´ì—ì„œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success(answer)

            with st.expander("ğŸ“„ ë‹µë³€ ê·¼ê±° ë¬¸ì„œ ë³´ê¸°"):
                for i, doc in enumerate(result["source_documents"]):
                    name = Path(doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼")).name
                    page = doc.metadata.get("page_number", "?")
                    st.markdown(f"**ë¬¸ì„œ {i+1}:** `{name}` (p.{page})")
                    preview = safe_unicode(doc.page_content[:500]) + "..."
                    st.text(preview)
        except Exception as e:
            st.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {safe_unicode(str(e))}")
