import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.chat_models import ChatOpenAI
import tempfile
import shutil

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# 2. LangChain ëª¨ë¸ ì •ì˜
embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)
llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)
chain = load_qa_chain(llm, chain_type="stuff")

# 3. ì¸ì½”ë”© ì•ˆì „ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
def safe_text(text):
    return text.encode('utf-8', errors='ignore').decode('utf-8')

# 4. ë²¡í„° DB ìƒì„± í•¨ìˆ˜ (ìºì‹± í¬í•¨)
@st.cache_resource(show_spinner="ì„ë² ë”© ë¡œë”© ì¤‘...")
def load_or_create_vector_db():
    if os.path.exists("data/index.faiss") and os.path.exists("data/index.pkl"):
        return FAISS.load_local("data/index", embedding_model, allow_dangerous_deserialization=True)
    else:
        docs = []
        for filename in os.listdir("data"):
            if filename.endswith(".pdf"):
                loader = PyPDFLoader(os.path.join("data", filename))
                docs.extend(loader.load())

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs_split = splitter.split_documents(docs)
        contents = [safe_text(doc.page_content) for doc in docs_split]

        vectors = embedding_model.embed_documents(contents)
        db = FAISS.from_documents(docs_split, embedding_model)
        db.save_local("data/index")
        return db

# 5. Streamlit UI ì„¤ì •
st.set_page_config(page_title="ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ë™ì¡°í•© ìƒë‹´ì‚¬", page_icon="ğŸ¤–")
st.markdown("""
    # ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ë™ì¡°í•© ìƒë‹´ì‚¬
    ì•ˆë…•í•˜ì„¸ìš”! ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, ë…¸ì¡° ê´€ë ¨ ìë£Œì—ì„œ ìë™ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
""")

# 6. ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë° ì‘ë‹µ ì²˜ë¦¬
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ìƒì¡° ì§€ì› ëŒ€ìƒì´ ëˆ„êµ°ê°€ìš”?")
if query:
    db = load_or_create_vector_db()
    docs = db.similarity_search(query, k=3)
    response = chain.run(input_documents=docs, question=query)
    st.markdown("---")
    st.subheader("ë‹µë³€")
    st.write(response)
