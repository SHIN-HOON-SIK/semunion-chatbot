import os
import sys
import io
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# âœ… ì‹œìŠ¤í…œ ì¶œë ¥ ì¸ì½”ë”© ì„¤ì • (UnicodeEncodeError ë°©ì§€)
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜ (í…ìŠ¤íŠ¸ ë‚´ íŠ¹ìˆ˜ë¬¸ìë‚˜ ë¹„ASCII ë¬¸ì ì •ë¦¬ ë° ì¸ì½”ë”© ê°•í™”)
def clean_text(text):
    if isinstance(text, str):
        # UTF-8ë¡œ ì¸ì½”ë”©í•˜ë˜, ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ í•´ë‹¹ ë¬¸ìë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.
        # ë‹¤ì‹œ UTF-8ë¡œ ë””ì½”ë”©í•˜ì—¬ ìœ ë‹ˆì½”ë“œ ë¬¸ìì—´ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        cleaned = text.encode("utf-8", errors="ignore").decode("utf-8")
        # ì¶”ê°€ì ìœ¼ë¡œ, APIë¡œ ì „ë‹¬ë  ë•Œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¹„í‘œì¤€ ê³µë°± ë¬¸ì ë“±ì„ ì œê±°í•˜ê³ 
        # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        return ' '.join(cleaned.split()).strip()
    return "" # ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

# ğŸ”‘ OpenAI API í‚¤ í™•ì¸
openai_api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'OPENAI_API_KEY' í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Streamlit secretsì— í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# ğŸŒ ì‚¬ìš©ì UI (ìš”ì²­í•˜ì‹  ë‚´ìš© ê·¸ëŒ€ë¡œ ì ìš©)
st.image("1.png", width=110)
st.markdown("<h1 style='display:inline-block; margin-left:10px;'>ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ì¡° ìƒë‹´ì‚¬</h1>", unsafe_allow_html=True)
st.write("ë…¸ì¡° ê´€ë ¨ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•´ë³´ì„¸ìš”.")


# ğŸ“‚ PDF ë¬¸ì„œ ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹± ì ìš©)
@st.cache_data(show_spinner="PDF ë¬¸ì„œ ë¡œë”© ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
def load_and_process_documents(file_paths):
    all_documents = []
    for path in file_paths:
        try:
            loader = PyPDFLoader(path)
            documents = loader.load()
            all_documents.extend(documents)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: '{path}' íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ({e})")
            continue

    if not all_documents:
        st.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(all_documents)

    # ëª¨ë“  í…ìŠ¤íŠ¸ ì²­í¬ì— clean_text ì ìš©
    cleaned_contents = [clean_text(doc.page_content) for doc in texts]

    return cleaned_contents, texts

# ğŸ§  ì„ë² ë”© + ë²¡í„° DB ìƒì„± í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹± ì ìš©)
@st.cache_resource(show_spinner="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
def create_vector_db_and_retriever(cleaned_contents, openai_key):
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
        db = FAISS.from_texts(cleaned_contents, embeddings)
        return db.as_retriever()
    except Exception as e:
        st.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. API í‚¤ì™€ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

# ---
# ë©”ì¸ ë¡œì§ ì‹œì‘
# ---

# ë¡œë“œí•  PDF íŒŒì¼ ê²½ë¡œ ëª©ë¡
pdf_file_paths = [
    "./data/25_government_agenda.pdf",
    "./data/respect_union_agenda.pdf",
    "./data/external_audit_preparation.pdf"
]

# 1. ë¬¸ì„œ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ìºì‹± ì ìš©)
cleaned_contents, original_texts = load_and_process_documents(pdf_file_paths)

# 2. ë²¡í„° DB ë° ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± (ìºì‹± ì ìš©)
retriever = create_vector_db_and_retriever(cleaned_contents, openai_api_key)

# 3. ì±—ë´‡ ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(openai_api_key=openai_api_key, temperature=0, model_name="gpt-4o"),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# ğŸ’¬ ì‚¬ìš©ì ì…ë ¥
query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?", placeholder="ì˜ˆ: ì™¸ë¶€ íšŒê³„ê°ì‚¬ ì¤€ë¹„ ë‚´ìš©ì€?")

if query:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            result = qa_chain({"query": query})
            st.success(result["result"])

        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ({e})")

    # ğŸ” ì°¸ì¡° ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
    with st.expander("ğŸ“ ê´€ë ¨ ë¬¸ì„œ ë³´ê¸°"):
        if "source_documents" in result and result["source_documents"]:
            for i, doc in enumerate(result["source_documents"]):
                source_info = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                st.markdown(f"**ë¬¸ì„œ {i+1}:** `{source_info}`")
                st.write(doc.page_content[:1000])
        else:
            st.info("ì´ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
