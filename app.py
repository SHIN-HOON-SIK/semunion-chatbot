# -*- coding: utf-8 -*-

import os
import sys
import re
import hashlib
from pathlib import Path

import streamlit as st
from PyPDF2 import PdfReader
from pptx import Presentation
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --------------------------------------------------------------------------
# [1. ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜]
# --------------------------------------------------------------------------

if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

def safe_unicode(text: str) -> str:
    """ì•ˆì „í•˜ê²Œ ìœ ë‹ˆì½”ë“œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    if not isinstance(text, str):
        text = str(text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")

def clean_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì œì–´ ë¬¸ì ë“± ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜"""
    if not isinstance(text, str):
        return ""
    text = text.replace("\x00", "")
    text = re.sub(r"[\u0000-\u001F\u007F-\u009F]", "", text)
    text = re.sub(r"[\ud800-\udfff]", "", text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore").strip()

def extract_text_from_pdf(path: Path) -> str:
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        reader = PdfReader(str(path))
        pages = [clean_text(page.extract_text() or "") for page in reader.pages]
        return "\n".join(pages)
    except Exception as e:
        st.warning(f"[PDF ì¶”ì¶œ ì‹¤íŒ¨] {path.name}: {e}")
        return ""

def extract_text_from_pptx(path: Path) -> str:
    """PPTX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        prs = Presentation(str(path))
        slides = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    slides.append(clean_text(shape.text))
        return "\n".join(slides)
    except Exception as e:
        st.warning(f"[PPTX ì¶”ì¶œ ì‹¤íŒ¨] {path.name}: {e}")
        return ""

def compute_file_hash(file_paths):
    """íŒŒì¼ ëª©ë¡ì˜ í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ íŒŒì¼ ë³€ê²½ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""
    hash_md5 = hashlib.md5()
    for path in sorted(file_paths):
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
    return hash_md5.hexdigest()


# --------------------------------------------------------------------------
# [2. LangChain ë° AI ëª¨ë¸ ê´€ë ¨ ê¸°ëŠ¥ ì •ì˜]
# --------------------------------------------------------------------------

@st.cache_resource
def load_all_documents_with_hash(file_paths, file_hash):
    """ì§€ì •ëœ ê²½ë¡œì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™€ LangChain Document ê°ì²´ë¡œ ë³€í™˜"""
    documents = []
    for path in file_paths:
        text = ""
        if path.suffix == ".pdf":
            text = extract_text_from_pdf(path)
        elif path.suffix == ".pptx":
            text = extract_text_from_pptx(path)
        
        if text.strip():
            doc = Document(page_content=text, metadata={"source": str(path.name)})
            documents.append(doc)
        else:
            st.warning(f"[ì‚­ì œ] {path.name} ì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return documents

@st.cache_resource
def split_documents_into_chunks(_documents):
    """ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• """
    total_length = sum(len(doc.page_content) for doc in _documents)
    avg_length = total_length // len(_documents) if _documents else 0
    chunk_size, overlap = (1500, 300) if avg_length > 6000 else (1000, 200) if avg_length > 3000 else (700, 200)
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    return splitter.split_documents(_documents)

@st.cache_resource
def create_vector_store(_chunks, _embedding_model):
    """ë¶„í• ëœ ì²­í¬ë¡œë¶€í„° FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±"""
    try:
        return FAISS.from_documents(_chunks, _embedding_model)
    except Exception as e:
        st.error(f"âŒ FAISS ë²¡í„° DB ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {safe_unicode(str(e))}")
        st.stop()

# LLMì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
QA_SYSTEM_PROMPT = """
ë„ˆëŠ” ë°˜ë“œì‹œ PDF/PPTX ë¬¸ì„œì— í¬í•¨ëœ ë‚´ìš©ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µí•´ì•¼ í•´.
ë¬¸ì„œì— ëª…ì‹œì  ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ì• ë§¤í•œ ê²½ìš° 'ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•´.
"""

# ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
QA_QUESTION_PROMPT = PromptTemplate(
    template=QA_SYSTEM_PROMPT + "\n\n{context}\n\nì§ˆë¬¸: {question}\në‹µë³€:",
    input_variables=["context", "question"]
)

@st.cache_resource
def initialize_qa_chain(all_paths, api_key):
    """ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•˜ì—¬ QA ì²´ì¸ì„ ìƒì„±"""
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    llm = ChatOpenAI(openai_api_key=api_key, model_name="gpt-4o", temperature=0)
    file_hash = compute_file_hash(all_paths)
    docs = load_all_documents_with_hash(all_paths, file_hash)
    if not docs:
        st.error("ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()
    
    chunks = split_documents_into_chunks(docs)

    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • (BM25 + FAISS)
    bm25_retriever = BM25Retriever.from_documents(chunks)
    bm25_retriever.k = 20

    faiss_vectorstore = create_vector_store(chunks, embeddings)
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 20})
    
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=[0.5, 0.5]
    )

    # --- ë‹¤ì¤‘ ì§ˆë¬¸ ìƒì„± ê¸°ëŠ¥ (ì•ˆì •ì ì¸ ê¸°ë³¸ ë²„ì „ ì‚¬ìš©) ---
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=ensemble_retriever, llm=llm
    )
    
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=multi_query_retriever,
        chain_type_kwargs={"prompt": QA_QUESTION_PROMPT},
        return_source_documents=True
    )

# --------------------------------------------------------------------------
# [3. Streamlit ì•± ì‹¤í–‰ ë¶€ë¶„]
# --------------------------------------------------------------------------

# [API í‚¤ ì„¤ì •]
openai_api_key = os.getenv("OPENAI_API_KEY", "").strip()
if not openai_api_key:
    try:
        openai_api_key = st.secrets["OPENAI_API_KEY"]
    except Exception:
        st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.")
        st.stop()

# [í˜ì´ì§€ ì„¤ì • ë° ì œëª©]
st.set_page_config(page_title="ì‚¼ì„±ì „ê¸° ì¢…ì¤‘ë…¸ì¡° ìƒë‹´ì‚¬", layout="centered", page_icon="logo_union_hands.png")
st.markdown("""
<div style='display: flex; align-items: center; justify-content: center; gap: 10px; margin-bottom: 10px;'>
    <img src="https://raw.githubusercontent.com/SHIN-HOON-SIK/semunion-chatbot/main/logo_union_hands.png" width="50"/>
    <h1 style='color: #0d1a44; margin: 0;'>ì‚¼ì„±ì „ê¸° ì¢…ì¤‘ë…¸ì¡° ìƒë‹´ì‚¬</h1>
</div>
""", unsafe_allow_html=True)
st.write("ë…¸ì¡° ì§‘í–‰ë¶€ì—ì„œ ë“±ë¡í•œ PDF ë° PPTX ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

# [ë¬¸ì„œ ê²½ë¡œ ì„¤ì •]
base_dir = Path(__file__).parent
data_dir = base_dir / "data"
doc_files = ["policy_agenda_250627.pdf", "union_meeting_250704.pdf", "SEMUNION_DATA_BASE.pdf", "SEMUNION_DATA_BASE.pptx"]
doc_paths = [data_dir / name for name in doc_files if (data_dir / name).exists()]

# [QA ì²´ì¸ ì´ˆê¸°í™”]
try:
    qa_chain = initialize_qa_chain(all_paths=doc_paths, api_key=openai_api_key)
except Exception as e:
    st.error(f"âš ï¸ ì•± ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# [ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë° ë‹µë³€ ì²˜ë¦¬]
user_query = st.text_input("ê¶ê¸ˆí•˜ì‹  ë‚´ìš©ì€ ì•„ë˜ ì°½ì— ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!", placeholder="ì—¬ê¸°ì— ìµœëŒ€í•œ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")

if user_query.strip():
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            result = qa_chain.invoke({"query": user_query})
            answer = result["result"]
            
            if not answer or "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in answer:
                st.info("ì£„ì†¡í•˜ì§€ë§Œ ì—…ë¡œë“œëœ ë¬¸ì„œ ë‚´ì—ì„œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë˜ëŠ” ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
            else:
                st.success(answer)

            with st.expander("ğŸ“„ ë‹µë³€ ê·¼ê±° ë¬¸ì„œ"):
                source_docs = result.get("source_documents", [])
                if source_docs:
                    for i, doc in enumerate(source_docs):
                        name = Path(doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼")).name
                        st.markdown(f"**ë¬¸ì„œ {i+1}:** `{name}`")
                        preview = doc.page_content[:500] + "..."
                        st.text(preview)
                else:
                    st.write("ë‹µë³€ì— ëŒ€í•œ ê·¼ê±° ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
