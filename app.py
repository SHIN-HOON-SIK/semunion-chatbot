# union_chatbot_app.py
# -*- coding: utf-8 -*-

import os
import re
import hashlib
import sys
from pathlib import Path
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.schema import HumanMessage, Document

# âœ… í™˜ê²½: Windows ì¸ì½”ë”© ì•ˆì „ ì²˜ë¦¬
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")
    sys.stderr.reconfigure(encoding="utf-8")

# âœ… ìœ ë‹ˆì½”ë“œ ì •ì œ í•¨ìˆ˜
def safe_unicode(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")

# âœ… PDF í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
def clean_text(text):
    text = text.replace("\x00", "")
    text = re.sub(r"[\u0000-\u001F\u007F-\u009F]", "", text)  # ì œì–´ë¬¸ì ì œê±°
    text = re.sub(r"[\ud800-\udfff]", "", text)  # surrogate ë¬¸ì ì œê±°
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore").strip()

# âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(path):
    try:
        reader = PdfReader(str(path))
        pages = [clean_text(page.extract_text() or "") for page in reader.pages]
        return "\n".join(pages)
    except Exception as e:
        st.warning(f"[PDF ì¶”ì¶œ ì˜¤ë¥˜] {path.name}: {e}")
        return ""

# âœ… íŒŒì¼ ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•œ í•´ì‹œ
def compute_file_hash(file_paths):
    h = hashlib.md5()
    for path in sorted(file_paths):
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                h.update(chunk)
    return h.hexdigest()

# âœ… ë¬¸ì„œ ë¡œë”©
@st.cache_resource
def load_all_documents_with_hash(pdf_paths, file_hash):
    docs = []
    for path in pdf_paths:
        text = extract_text_from_pdf(path)
        if text.strip():
            docs.append(Document(page_content=text, metadata={"source": str(path.name)}))
        else:
            st.warning(f"âš ï¸ {path.name} í…ìŠ¤íŠ¸ ì—†ìŒ â†’ ìƒëµ")
    return docs

# âœ… í…ìŠ¤íŠ¸ ë¶„í• 
@st.cache_resource
def split_documents_into_chunks(_documents):
    total_length = sum(len(doc.page_content) for doc in _documents)
    avg = total_length // len(_documents) if _documents else 0
    if avg > 6000:
        chunk_size, overlap = 1500, 300
    elif avg > 3000:
        chunk_size, overlap = 1000, 200
    else:
        chunk_size, overlap = 700, 100
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    return splitter.split_documents(_documents)

# âœ… ë²¡í„° DB ìƒì„±
@st.cache_resource
def create_vector_store(_chunks, _embedding_model):
    try:
        for doc in _chunks:
            doc.page_content = safe_unicode(doc.page_content)
        return FAISS.from_documents(_chunks, _embedding_model)
    except Exception as e:
        st.error(f"âŒ ë²¡í„° DB ìƒì„± ì˜¤ë¥˜: {safe_unicode(str(e))}")
        st.stop()

# âœ… QA ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_qa_chain():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    full_paths = [PDF_FILES_DIR / name for name in PDF_FILES]
    file_hash = compute_file_hash(full_paths)
    docs = load_all_documents_with_hash(full_paths, file_hash)
    if not docs:
        st.error("âŒ ìœ íš¨í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    chunks = split_documents_into_chunks(docs)
    db = create_vector_store(chunks, embeddings)
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4o", temperature=0)
    return RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(search_type="similarity", search_kwargs={"k": 6}), return_source_documents=True)

# âœ… ì§ˆë¬¸ í™•ì¥ GPT
@st.cache_resource
def get_query_expander():
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4o", temperature=0)
    def expand(query):
        try:
            prompt = HumanMessage(content=safe_unicode(
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ PDF ë‚´ìš©ê³¼ ë§¤ì¹­ë˜ë„ë¡ ëª…í™•í•˜ê²Œ ë‹¤ì‹œ ì‘ì„±í•´ì¤˜.\n"
                f"ì§ˆë¬¸: {query}"
            ))
            res = llm.invoke([prompt])
            return safe_unicode(res.content.strip())
        except Exception as e:
            st.warning(f"â• ì§ˆë¬¸ í™•ì¥ ì˜¤ë¥˜: {safe_unicode(str(e))}")
            return query
    return expand

# âœ… OpenAI API í‚¤ ë¡œë”©
try:
    openai_api_key = st.secrets["OPENAI_API_KEY"].strip()
except Exception:
    openai_api_key = os.getenv("OPENAI_API_KEY", "").strip()
if not openai_api_key or not openai_api_key.startswith("sk-"):
    st.error("âŒ OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ëˆ„ë½ë¨ (sk-ë¡œ ì‹œì‘í•´ì•¼ í•¨)")
    st.stop()

# âœ… ì„¤ì •
BASE_DIR = Path(__file__).parent
PDF_FILES_DIR = BASE_DIR / "data"
PDF_FILES = [
    "policy_agenda_250627.pdf",
    "union_meeting_250704.pdf",
    "SEMUNION_DATA_BASE.pdf"
]

# âœ… UI êµ¬ì„±
st.set_page_config(page_title="ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ì¡° ìƒë‹´ì‚¬", layout="centered", page_icon="ğŸ¤–")
st.title("ğŸ¤– ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ì¡° ìƒë‹´ì‚¬")
st.write("PDF ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë…¸ì¡° ë° íšŒì‚¬ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

# âœ… ì²´ì¸ ì´ˆê¸°í™”
try:
    query_expander = get_query_expander()
    qa_chain = initialize_qa_chain()
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {safe_unicode(str(e))}")
    st.stop()

# âœ… ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬
raw_query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?", placeholder="ì˜ˆ: ì‹ í›ˆì‹ ìœ„ì›ì¥ì— ëŒ€í•´ ì•Œë ¤ì¤˜")
query = query_expander(raw_query.strip()) if raw_query.strip() else ""

if query:
    with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            result = qa_chain.invoke({"query": query})
            answer = safe_unicode(result["result"])
            if not answer or "ì—†" in answer:
                st.info("ğŸ›‘ ë¬¸ì„œ ë‚´ í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì§‘í–‰ë¶€ì— ì—…ë°ì´íŠ¸ ìš”ì²­ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            else:
                st.success(answer)
            
            with st.expander("ğŸ“ ë‹µë³€ ê·¼ê±° ë¬¸ì„œ ë³´ê¸°"):
                for i, doc in enumerate(result["source_documents"]):
                    name = Path(doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")).name
                    preview = safe_unicode(doc.page_content.strip())[:400] + "..."
                    st.markdown(f"**ë¬¸ì„œ {i+1}:** `{name}`")
                    st.text(preview)
                    st.markdown("---")
        except Exception as e:
            st.error(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {safe_unicode(str(e))}")

