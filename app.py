import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# --- CONFIGURATION ---
# Load OpenAI API key from Streamlit secrets
# Fallback to environment variable if not found in secrets
try:
    openai_api_key = st.secrets["OPENAI_API_KEY"]
except (KeyError, AttributeError):
    openai_api_key = os.getenv("OPENAI_API_KEY")

# Stop the app if the API key is not set
if not openai_api_key:
    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit secretsì— 'OPENAI_API_KEY'ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

# Define paths to your PDF files
PDF_FILES_DIR = "./data/"
PDF_FILES = [
    "2025_government_labor_policy_agenda.pdf",
    "seme_union_meeting_250704.pdf"
]

# --- UI SETUP ---
st.set_page_config(page_title="ë…¸ì¡° ìƒë‹´ ì±—ë´‡", page_icon="ğŸ¤–")

# Custom CSS for better styling
st.markdown("""
<style>
    .stApp {
        background-color: #f0f2f6;
    }
    .stSpinner > div > div {
        border-top-color: #0062ff;
    }
    .stSuccess {
        background-color: #e6f7ff;
        border: 1px solid #91d5ff;
        border-radius: 8px;
        color: #0050b3;
    }
</style>
""", unsafe_allow_html=True)


# App header
st.image("1.png", width=110)
st.markdown("<h1 style='display:inline-block; vertical-align:middle; margin-left:10px; color: #0d1a44;'>ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ë™ì¡°í•© ìƒë‹´ì‚¬</h1>", unsafe_allow_html=True)
st.write("ì•ˆë…•í•˜ì„¸ìš”! ë…¸ì¡° ê´€ë ¨ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤. ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
st.markdown("---")


# --- DATA LOADING AND PROCESSING ---

@st.cache_resource
def load_all_documents(pdf_paths):
    """
    Loads documents from a list of PDF file paths.
    Caches the result to avoid reloading on every interaction.
    """
    all_docs = []
    for path in pdf_paths:
        if os.path.exists(path):
            try:
                loader = PyPDFLoader(path)
                all_docs.extend(loader.load())
            except Exception as e:
                st.warning(f"'{path}' íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            st.warning(f"'{path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    return all_docs

@st.cache_resource
def split_documents_into_chunks(documents):
    """
    Splits loaded documents into smaller chunks for processing.
    Caches the result.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    texts = text_splitter.split_documents(documents)
    # The content is already a clean string, no need for encode/decode
    return texts

@st.cache_resource
def create_vector_store(_texts, _embedding_model):
    """
    Creates a FAISS vector store from text chunks and an embedding model.
    The '_' prefix in args tells Streamlit to hash the object's contents for caching.
    """
    try:
        vector_store = FAISS.from_documents(_texts, _embedding_model)
        return vector_store
    except Exception as e:
        st.error(f"ë²¡í„° DB ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

# --- QA CHAIN SETUP ---

@st.cache_resource
def initialize_qa_chain():
    """
    Initializes all components and builds the RetrievalQA chain.
    This function orchestrates the entire setup process and caches the final chain.
    """
    # 1. Initialize embedding model
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # 2. Load documents
    full_pdf_paths = [os.path.join(PDF_FILES_DIR, f) for f in PDF_FILES]
    documents = load_all_documents(full_pdf_paths)
    if not documents:
        st.error("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. 'data' í´ë”ì— PDF íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    # 3. Split documents
    text_chunks = split_documents_into_chunks(documents)

    # 4. Create Vector DB
    db = create_vector_store(text_chunks, embeddings)

    # 5. Create Retriever
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    # 6. Initialize LLM and create QA chain
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4o", temperature=0)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    return qa_chain

# --- MAIN APPLICATION LOGIC ---

# Initialize the QA chain
try:
    qa_chain = initialize_qa_chain()
except Exception as e:
    st.error(f"ì±—ë´‡ ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()


# Get user input
query = st.text_input(
    "ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
    placeholder="ì˜ˆ: 7ì›” ì •ê¸°í˜‘ì˜ ì£¼ìš” ì˜ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    key="query_input"
)

if query:
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        try:
            # Run the QA chain with the user's query
            result = qa_chain.invoke({"query": query})

            # Display the answer
            st.success(result["result"])

            # Display source documents in an expander
            with st.expander("ğŸ“ ë‹µë³€ ê·¼ê±° ë¬¸ì„œ ë³´ê¸°"):
                for i, doc in enumerate(result["source_documents"]):
                    source_name = os.path.basename(doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜'))
                    st.markdown(f"**ë¬¸ì„œ {i+1}:** `{source_name}` (í˜ì´ì§€: {doc.metadata.get('page', 'N/A') + 1})")
                    # Display a snippet of the page content
                    st.write(f'"{doc.page_content.strip()[:500]}..."')
                    st.markdown("---")

        except Exception as e:
            st.error(f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

