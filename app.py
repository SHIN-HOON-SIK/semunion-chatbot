# -*- coding: utf-8 -*-

# --- [0. ë¹„ë™ê¸° ë¬¸ì œ í•´ê²°] ---
import nest_asyncio
nest_asyncio.apply()

import os
import sys
import re
import hashlib
from pathlib import Path
import json
from datetime import datetime, date

# --- [í† í° ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬] ---
import tiktoken

import streamlit as st
from PyPDF2 import PdfReader
from pptx import Presentation
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever
import streamlit.components.v1 as components

# --------------------------------------------------------------------------
# [1. ì¼ì¼ ì‚¬ìš©ëŸ‰ ì œí•œ ì„¤ì •]
# --------------------------------------------------------------------------
DAILY_TOKEN_LIMIT = 20_000
USAGE_FILE = Path(__file__).parent / "usage_data.json"

def load_usage_data():
    today_str = str(date.today())
    try:
        if USAGE_FILE.exists():
            with open(USAGE_FILE, 'r') as f: data = json.load(f)
            if data.get("date") == today_str: return data.get("tokens_used", 0)
    except (json.JSONDecodeError, FileNotFoundError): pass
    save_usage_data(0)
    return 0

def save_usage_data(tokens_used):
    with open(USAGE_FILE, 'w') as f:
        json.dump({"date": str(date.today()), "tokens_used": tokens_used}, f)

def count_tokens(text: str) -> int:
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text, disallowed_special=()))
    except Exception: return len(text) // 2

# --------------------------------------------------------------------------
# [ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜]
# --------------------------------------------------------------------------
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")
    sys.stderr.reconfigure(encoding="utf-8")

def safe_unicode(text: str) -> str:
    if not isinstance(text, str): text = str(text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore").strip()
def clean_text(text):
    if not isinstance(text, str): return ""
    text = text.replace("\x00", ""); text = re.sub(r"[-\u001F\u007F-\u009F]", "", text)
    text = re.sub(r"[\ud800-\udfff]", "", text)
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore").strip()
def extract_text_from_pdf(path: Path) -> str:
    try:
        reader = PdfReader(str(path))
        return "\n".join(clean_text(page.extract_text() or "") for page in reader.pages)
    except Exception as e: st.warning(f"[PDF ì¶”ì¶œ ì‹¤íŒ¨] {path.name}: {e}"); return ""
def extract_text_from_pptx(path: Path) -> str:
    try:
        prs = Presentation(str(path))
        return "\n".join(clean_text(shape.text) for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text"))
    except Exception as e: st.warning(f"[PPTX ì¶”ì¶œ ì‹¤íŒ¨] {path.name}: {e}"); return ""
def extract_text_from_json(path: Path) -> list[Document]:
    documents = []
    try:
        with open(path, 'r', encoding='utf-8') as f: data = json.load(f)
        for item in data:
            if not isinstance(item, dict): continue
            item_type = item.get("type", "ì •ë³´")
            item_name = item.get("name") or item.get("ì¡°í•©ì›_í˜•íƒœ") or item.get("category")
            summary = f"ìœ í˜•: {item_type}, ì´ë¦„: {item_name}\n"
            full_content = json.dumps(item, ensure_ascii=False, indent=2)
            documents.append(Document(page_content=summary + full_content, metadata={"source": str(path.name), "type": item_type, "name": item_name}))
        return documents
    except Exception as e: st.warning(f"[JSON ì¶”ì¶œ ì‹¤íŒ¨] {path.name}: {e}"); return []
def compute_file_hash(file_paths):
    hash_md5 = hashlib.md5()
    for path in sorted(file_paths):
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
    return hash_md5.hexdigest()

@st.cache_resource
def load_all_documents_with_hash(file_paths, file_hash):
    documents = []
    for path in file_paths:
        if path.suffix == ".json" or ("json" in path.name.lower() and path.suffix == ".txt"):
            json_docs = extract_text_from_json(path)
            for doc in json_docs:
                doc.page_content = "[ê°€ì¥ ì •í™•í•œ ìµœì‹  ì •ë³´ ì¶œì²˜: JSON ë°ì´í„°ë² ì´ìŠ¤]\n\n" + doc.page_content
            documents.extend(json_docs)
        elif path.suffix == ".pdf":
            text = extract_text_from_pdf(path)
            if text.strip():
                documents.append(Document(page_content="[ì°¸ê³  ìë£Œ]\n\n" + text, metadata={"source": str(path.name)}))
        elif path.suffix == ".pptx":
            text = extract_text_from_pptx(path)
            if text.strip():
                documents.append(Document(page_content="[ì°¸ê³  ìë£Œ]\n\n" + text, metadata={"source": str(path.name)}))
            
    if not documents:
        st.error("ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. data í´ë”ë¥¼ ë§Œë“¤ê³  ê·¸ ì•ˆì— ë¬¸ì„œë“¤ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        st.stop()
    return documents

@st.cache_resource
def split_documents_into_chunks(_documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
    return splitter.split_documents(_documents)

def preprocess_query(query):
    # ì—°ê´€ ê²€ìƒ‰ì–´ ì‚¬ì „
    synonyms = {
        "í”¼í”ŒíŒ€": ["í”¼í”ŒíŒ€", "í”¼í”ŒíŒ€ ERê·¸ë£¹"],
    }
    original_query = query.strip()
    expanded_terms = set([original_query])
    for keyword, related_terms in synonyms.items():
        if keyword in original_query:
            for term in related_terms:
                expanded_terms.add(term)
    if len(expanded_terms) > 1:
        additional_terms = expanded_terms - {original_query}
        expanded_query = f"{original_query} ë˜ëŠ” {' ë˜ëŠ” '.join(additional_terms)}"
    else:
        expanded_query = original_query
    if not expanded_query.endswith(("ì— ëŒ€í•´", "ì— ëŒ€í•œ ì •ë³´", "?")):
         expanded_query += "ì— ëŒ€í•´"
    return expanded_query

# --- [ìˆ˜ì •ëœ QA_SYSTEM_PROMPT] ---
QA_SYSTEM_PROMPT = """
ë„ˆëŠ” ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AI ìƒë‹´ì‚¬ë‹¤. ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ìƒì„±í•´ì•¼ í•œë‹¤.
1. ì •ë³´ ìš°ì„ ìˆœìœ„ ì¤€ìˆ˜: 1ìˆœìœ„ [JSON], 2ìˆœìœ„ [PDF], 3ìˆœìœ„ [PPTX] ìˆœìœ¼ë¡œ ì •ë³´ë¥¼ ì‹ ë¢°í•˜ê³  ë‹µë³€ì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì•„ë¼.
2. ì •ë³´ ì¶©ëŒ ì‹œ í•´ê²°: ë¬¸ì„œ ê°„ ì •ë³´ê°€ ì¶©ëŒí•˜ë©´, ë°˜ë“œì‹œ ë” ë†’ì€ ìˆœìœ„ì˜ ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µí•´ì•¼ í•œë‹¤. (JSON > PDF > PPTX)
3. ì •ë³´ ë¶€ì¬ ì‹œ ì‘ë‹µ: ì–´ë–¤ ë¬¸ì„œì—ì„œë„ ì§ˆë¬¸ì— ëŒ€í•œ ëª…ì‹œì  ì–¸ê¸‰ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•íˆ ë‹µë³€í•´ë¼. ì¶”ì¸¡í•´ì„œ ë‹µë³€í•˜ì§€ ë§ˆë¼.
4. ë‹µë³€ í˜•ì‹: ì‚¬ìš©ìê°€ ì§§ì€ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•˜ë”ë¼ë„, ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì•„ ìµœëŒ€í•œ ìƒì„¸í•˜ê³  ì¹œì ˆí•œ ë¬¸ì¥ìœ¼ë¡œ ì™„ì„±í•´ì„œ ë‹µë³€í•´ë¼.
5. ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš°: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ì„¤ëª…ì´ë‚˜ ì •ì˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì§€ë§Œ, ê´€ë ¨ í‚¤ì›Œë“œê°€ ì–¸ê¸‰ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´, 'ì •ì˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì§€ë§Œ, ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì—ì„œ ì–¸ê¸‰ë˜ì—ˆìŠµë‹ˆë‹¤'ë¼ê³  ì•ˆë‚´í•˜ë©° í•´ë‹¹ ë¬¸ë§¥ì„ ì¸ìš©í•´ë¼.
"""
QA_QUESTION_PROMPT = PromptTemplate(template=QA_SYSTEM_PROMPT + "\n\n{context}\n\nì…ë ¥: {question}\në‹µë³€:", input_variables=["context", "question"])

@st.cache_resource
def initialize_qa_chain(all_paths, api_key):
    file_hash = compute_file_hash(all_paths)
    docs = load_all_documents_with_hash(all_paths, file_hash)
    chunks = split_documents_into_chunks(docs)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    llm = ChatGoogleGenerativeAI(google_api_key=api_key, model="gemini-1.5-flash", temperature=0)
    bm25_retriever = BM25Retriever.from_documents(chunks)
    bm25_retriever.k = min(20, len(chunks))
    faiss_vectorstore = FAISS.from_documents(chunks, embeddings)
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 10})
    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.6, 0.4])
    return RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=ensemble_retriever, chain_type_kwargs={"prompt": QA_QUESTION_PROMPT}, return_source_documents=True)

# --- [ì•± ì‹¤í–‰ ë¶€ë¶„] ---
google_api_key = os.getenv("GOOGLE_API_KEY", "").strip() or st.secrets.get("GOOGLE_API_KEY")
if not google_api_key:
    st.error("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Hugging Faceì˜ 'Settings > Repository secrets'ì— GOOGLE_API_KEYë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."); st.stop()

st.set_page_config(page_title="ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ì¡° AI ì§‘ì‚¬", layout="centered", page_icon="https://raw.githubusercontent.com/SHIN-HOON-SIK/semunion-chatbot/main/logo_union_hands.png")

# --- [ìµœì¢… ë””ìì¸ CSS ìŠ¤íƒ€ì¼] ---
st.markdown("""
<style>
    /* ì§ˆë¬¸ ì…ë ¥ì°½ í´ë¦­ ì‹œ êµµê³  ì§„í•œ íŒŒë€ìƒ‰ í…Œë‘ë¦¬ ì ìš© */
    div[data-testid="stChatInput"]:focus-within {
        border: 2px solid #005A9C !important;
        box-shadow: none !important;
    }
    /* ë‚´ë¶€ ì…ë ¥ì°½ì˜ ê¸°ë³¸ í¬ì»¤ìŠ¤ íš¨ê³¼ëŠ” ì™„ì „íˆ ì œê±° */
    div[data-testid="stChatInput"] textarea:focus,
    div[data-testid="stChatInput"] textarea:focus-visible {
        border: none !important;
        box-shadow: none !important;
        outline: none !important;
    }
    
    /* ì‚¬ìš©ì ì•„ë°”íƒ€(ì•„ì´ì½˜) ìˆ¨ê¸°ê¸° */
    div[data-testid="stChatMessage-user"] div[data-testid="stChatAvatar"] {
        display: none;
    }

    /* ì‚¬ìš©ì ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ ì˜¤ë¥¸ìª½ ì •ë ¬ */
    div[data-testid="stChatMessage-user"] {
        justify-content: flex-end;
    }

    /* ë§í’ì„  ê³µí†µ ìŠ¤íƒ€ì¼ */
    div[data-testid="stMarkdownContainer"] {
        border-radius: 0.75rem;
        padding: 1rem;
        border: none;
        word-wrap: break-word;
    }
    
    /* ë‹µë³€ ë§í’ì„  ë°°ê²½ìƒ‰ */
    div[data-testid="stChatMessage-assistant"] div[data-testid="stMarkdownContainer"] {
        background-color: #f1f3f5;
        color: #000;
    }

    /* ì§ˆë¬¸ ë§í’ì„  ë°°ê²½ìƒ‰ */
    div[data-testid="stChatMessage-user"] div[data-testid="stMarkdownContainer"] {
        background-color: #ffffff;
        border: 1px solid #e0e0e0;
        color: #000;
    }
</style>
""", unsafe_allow_html=True)

st.markdown("""
<div style='display: flex; align-items: center; justify-content: center; gap: 10px; margin-bottom: 20px;'>
    <img src="https://raw.githubusercontent.com/SHIN-HOON-SIK/semunion-chatbot/main/logo_union_hands.png" width="50"/>
    <h1 style='color: #0d1a44; margin: 0;'>ì‚¼ì„±ì „ê¸° ì¡´ì¤‘ë…¸ì¡° AI ì§‘ì‚¬</h1>
</div>
<p style='text-align: center;'>ì¡´ì¤‘ë…¸ì¡° ì§‘í–‰ë¶€ì—ì„œ ë“±ë¡í•œ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.</p>
""", unsafe_allow_html=True)

# --- [ë©”ì¸ ë¡œì§] ---
base_dir = Path(__file__).parent
data_dir = base_dir / "data"
if not data_dir.exists():
    st.error(f"'{data_dir.name}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); st.stop()
doc_paths = list(data_dir.glob("**/*.*"))

qa_chain = None
if not doc_paths:
    st.warning("data í´ë”ì— ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    try:
        qa_chain = initialize_qa_chain(all_paths=doc_paths, api_key=google_api_key)
    except Exception as e:
        st.error(f"ì•± ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"); st.stop()

if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar=message.get("avatar")):
        st.markdown(message["content"])

user_query = st.chat_input("ê¶ê¸ˆí•˜ì‹  ë‚´ìš©ì€ ì—¬ê¸°ì— ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!", key="user_input")
tokens_used_today = load_usage_data()

if user_query:
    # --- ì§ˆë¬¸ ê¸¸ì´ ì œí•œ ë¡œì§ ì¶”ê°€ ---
    if len(user_query) > 20:
        st.warning("ì‚¬ìš© ìš”ê¸ˆ ì¢…ëŸ‰ì œ ë¬¸ì œë¡œ ì§ˆë¬¸ì˜ ê¸¸ì´ë¥¼ ì œí•œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤.")
    else:
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)
        
        if tokens_used_today >= DAILY_TOKEN_LIMIT:
            limit_message = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì„¸ì…˜ì—ì„œ ë‹µë³€í•´ ë“œë¦´ ìˆ˜ ìˆëŠ” í† í°ì„ ëª¨ë‘ ì†Œì§„í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            st.warning(limit_message)
            st.session_state.messages.append({"role": "assistant", "content": limit_message, "avatar": "https://raw.githubusercontent.com/SHIN-HOON-SIK/semunion-chatbot/main/logo_union_hands.png"})
        elif qa_chain:
            with st.spinner("AI ì§‘ì‚¬ê°€ ë“±ë¡ëœ ìë£Œë¥¼ í›‘ì–´ë³´ê³  ìˆìŠµë‹ˆë‹¤... ğŸ‘€â³..."):
                try:
                    processed_query = preprocess_query(user_query)
                    result = qa_chain.invoke({"query": processed_query})
                    answer = result["result"]
                    
                    tokens_this_round = count_tokens(processed_query) + count_tokens(answer)
                    new_total_tokens = tokens_used_today + tokens_this_round
                    save_usage_data(new_total_tokens)
                    
                    assistant_avatar = "https://raw.githubusercontent.com/SHIN-HOON-SIK/semunion-chatbot/main/logo_union_hands.png"
                    response = "ì£„ì†¡í•˜ì§€ë§Œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³µì§œë¡œ ë§Œë“  ì±—ë´‡ì´ë¼ ëŠ¥ë ¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. T.T ë‹¤ë¥´ê²Œ ì§ˆë¬¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤." if not answer or "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in answer else answer
                    
                    st.session_state.messages.append({"role": "assistant", "content": response, "avatar": assistant_avatar})
                    with st.chat_message("assistant", avatar=assistant_avatar):
                        if "ì£„ì†¡í•˜ì§€ë§Œ" in response: st.info(response)
                        else: st.success(response)
                    
                    st.markdown(f"<p style='text-align: right; font-size: 0.8em; color: grey;'>ì´ë²ˆ ì‘ë‹µ: ì•½ {tokens_this_round:,} í† í° | ì„¸ì…˜ ì‚¬ìš©ëŸ‰: {new_total_tokens:,} / {DAILY_TOKEN_LIMIT:,} í† í°</p>", unsafe_allow_html=True)
                    
                    # --- [â˜… ìŠ¤í¬ë¡¤ ì œì–´ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€ â˜…] ---
                    scroll_script = """
                    <script>
                        window.setTimeout(function() {
                            const messages = window.parent.document.querySelectorAll('[data-testid="stChatMessage"]');
                            if (messages.length > 0) {
                                const lastMessage = messages[messages.length - 1];
                                lastMessage.scrollIntoView({ behavior: 'smooth', block: 'center', inline: 'nearest' });
                            }
                        }, 300);
                    </script>
                    """
                    components.html(scroll_script, height=0)

                except Exception as e:
                    st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        else:
            st.warning("data í´ë”ì— ë¬¸ì„œê°€ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- [ì•± í‘¸í„°] ---
st.markdown("""
<hr style="margin-top: 3em; margin-bottom: 1em;">
<div style="text-align: center; font-size: 0.85em; color: gray;">
    ìˆ˜ì›ì‹œ ì˜í†µêµ¬ ë§¤ì˜ë¡œ 159ë²ˆê¸¸ 19 ê´‘êµ ë” í¼ìŠ¤íŠ¸ ì§€ì‹ì‚°ì—…ì„¼í„°<br>
    ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸ 133-82-71927<br>
    ìœ„ì›ì¥: ì‹ í›ˆì‹ | ëŒ€í‘œë²ˆí˜¸: 010-9496-6517 | ì´ë©”ì¼: hoonsik79@hanmail.net
</div>
""", unsafe_allow_html=True)
